<p align="center">
    <a href="https://miccai2021.org/en/" alt="MICCAI Conference">
        <img src="https://img.shields.io/static/v1?label=MICCAI&message=2021&color=blue&logo=data:image/png;base64,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" /></a>
     <a href="https://arxiv.org/abs/2107.00977" alt="arXiv"><img src="http://img.shields.io/badge/arXiv-2107.00977-B31B1B.svg" /></a>
    <a href="https://github.com/HReynaud/UVT/blob/main/LICENSE" alt="MIT License"><img src="https://img.shields.io/badge/License-MIT-green" /></a>
</p>

# UVT
Ultrasound Video Transformers (UVT) for Cardiac Ejection Fraction Estimation. Code used for https://arxiv.org/abs/2107.00977

# Before using this repo
You will need to request access to the EchoNet dataset by completing the form on this page: https://echonet.github.io/dynamic/index.html#dataset
Once you have access to the data, download it and write the path of the "EchoNet-Dynamic" folder in the <code>dataset_path</code> variable in main.py.

# Train a network
Experiments can be launched from the [main.py](https://github.com/HReynaud/UVT/blob/main/main.py) file. Set the parameters directly in the code file and run the file to train the network. An example is ready to launch when running [main.py](https://github.com/HReynaud/UVT/blob/main/main.py).

# Test a network 
As for training, the test function is called from the [main.py](https://github.com/HReynaud/UVT/blob/main/main.py) file. An example is ready to launch when running [main.py](https://github.com/HReynaud/UVT/blob/main/main.py). To download the weights of the networks used in the paper, use the [download_weights.sh](https://github.com/HReynaud/UVT/blob/main/download_weights.sh) script. The network parameters for these weights are:

<table>
<tr>
    <th>Parameter</th>
    <th>Value</th>
  </tr>
  <tr>
    <td><code>latent_dim</code></td>
    <td>1024</td>
  </tr>
  <tr>
    <td><code>num_hidden_layers</code></td>
    <td>16</td>
  </tr>
  <tr>
    <td><code>intermediate_size</code></td>
    <td>8192</td>
  </tr>
  <tr>
    <td><code>use_full_videos</code></td>
    <td>True</td>
  </tr>
  <tr>
    <td><code>SDmode</code><sup>1</sup></td>
    <td>'reg' or 'cla'</td>
  </tr>
  <tr>
    <td><code>model_path</code><sup>1</sup></td>
    <td>./output/UVT_[R/M]_[REG/CLA]</td>
  </tr>
</table>
<sup>1</sup>Adapt these to the weight file in use
<br/>
<br/>

# Results
The network predicts the position of the ES and ED frames in a video of arbitrary length as well as the Left Ventricle Ejection Fraction.
![alt results](https://github.com/HReynaud/UVT/blob/main/images/example.png)

# Disclaimer
The code in ResNetAE.py is taken from the ResNetAE repo (https://github.com/farrell236/ResNetAE) and pruned to the minimum.
The training code is inspired by the echonet-dynamic repo (https://github.com/echonet/dynamic).

